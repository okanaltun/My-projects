{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy as sp\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve  \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOW:\n",
    "\n",
    "- This is a study that interests in detecting whether a review is fake or true on available yelp review data on Kaggle. The aim here is to construct a pipeline, strecth muscles for analytical thinking and sensible business approach. Therefore, I did not focus on model trials and relevant operations since they can be technically enhanced by including hyperparameter optimization, cross validation or so on... Rather than that, I approached to this issue in a way that woul help me construct a pipeline.\n",
    "\n",
    "- To go one step ahead, the outcome of this study might be used as a fake review detector which might create an alert for yelp to suggest identity control step to the user and a collection of a certain number of fake alerts might result in account suspension. \n",
    "\n",
    "- A beneficial side of this study is that there are no features available in the original dataset which requires us to think outside-the-box and generate ideas, use NLP operations and so forth..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval & Examination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./labeled_yelp/yelp.csv\").dropna()\n",
    "\n",
    "rows = len(data)\n",
    "print(f\"Number of rows: {rows}\")\n",
    "\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's change the columns names into meaningful ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"user_id\",\"product_id\",\"rating\",\"date\",\"review\",\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Target variable label is constructed as categorical -1 & 1. Let's switch it to binary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"label\"] = data[\"label\"].apply(lambda x: 1 if x== -1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The date column is constructed poorly, but since the dataset comes with limited number of variables, we might need the date to extract features. To do that, let's correct date column in a proper form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_series = list(data[\"date\"])\n",
    "date_list = [[int(date.split('/')[0]),int(date.split('/')[1]),int(date.split('/')[2])] for date in date_series]\n",
    "data['date'] = [datetime.date(tarih[2], tarih[0], tarih[1]) for tarih in date_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's save the corrected data as pickle in case we need it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"yelp_initial.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Data Examination & Feature Generation:\n",
    "\n",
    "- Since dataset comes with almost no feature, we will be deriving potentially explanatory features for later use.\n",
    "- Whilst doing that, we will examine the relation between the derived feature and the target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's start with feature generation brain storming, below are some ideas that I suggest as they might be explanatory to target variable:\n",
    "\n",
    "- length of the review: Fake reviews might tend to be shorter than the true ones.\n",
    "- average rating of the restaurant \n",
    "- average rating of the users\n",
    "- number of reviews that the restaurant received\n",
    "- number of unique users that the restaurant received review from\n",
    "- number of reviews that the user made\n",
    "- number of unique restaurants that the user made review to\n",
    "- time between the first and the last time a restaurant got a review\n",
    "- time between the first and the last time a user made a review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In addition to all those, we might have a gold mine in terms of fake review detection: the review itself. It is a text area which requires treatment to be available for prediction\n",
    "\n",
    "- Text Correction\n",
    "- Word Cloud Analysis\n",
    "- Sentiment Analysis & Its Harmony with the rating\n",
    "- Tagging the text & Analysis of Sentence Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_contractions(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"are not\",\n",
    "\"aren't\": \"am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I had\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it has\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review1\"] = data[\"review\"].apply(correct_contractions)\n",
    "data[\"tokens\"] = data[\"review1\"].apply(lambda x: nltk.RegexpTokenizer(r'\\w+').tokenize(x))\n",
    "\n",
    "data[\"cleaned_tokens\"] = data[\"tokens\"].apply(lambda x: [token for token in x if token.lower() not in ['â']])\n",
    "\n",
    "data[\"review_corrected\"] = data[\"cleaned_tokens\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Length of review:\n",
    "\n",
    "data[\"len_review\"] = data[\"review\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average rating of the restaurant\n",
    "per_prod = data.groupby(\"product_id\")[\"rating\"].mean()\n",
    "data_1 = data.merge(per_prod, how=\"left\", on=\"product_id\").rename(columns={\"rating_x\":\"rating\",\"rating_y\":\"avg_rating_prod\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average rating from the user\n",
    "per_user = data_1.groupby(\"user_id\")[\"rating\"].mean()\n",
    "data_1 = data_1.merge(per_user,how=\"left\",on=\"user_id\").rename(columns={\"rating_x\":\"rating\",\"rating_y\":\"avg_rating_user\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of ratings for the restaurant\n",
    "num_prod = data_1.groupby(\"product_id\")[\"rating\"].count()\n",
    "data_1 = data_1.merge(num_prod, how=\"left\",on=\"product_id\").rename(columns={\"rating_x\":\"rating\",\"rating_y\":\"num_review_prod\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of unique users that the restaurant received review from\n",
    "prod_unique = pd.DataFrame(data_1.groupby(\"product_id\")[\"user_id\"].nunique()).reset_index()\n",
    "data_1 = data_1.merge(prod_unique,on=\"product_id\",how=\"left\")\n",
    "\n",
    "data_1 = data_1.rename(columns={\"user_id_y\":\"prod_unique_review\",\n",
    "                                \"user_id_x\":\"user_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of ratings from the user\n",
    "num_user = data_1.groupby(\"user_id\")[\"rating\"].count()\n",
    "data_1 = data_1.merge(num_user,how=\"left\",on=\"user_id\").rename(columns={\"rating_x\":\"rating\",\"rating_y\":\"num_review_user\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of distinct restaurants that the user rated\n",
    "user_unique = pd.DataFrame(data_1.groupby(\"user_id\")[\"product_id\"].nunique()).reset_index()\n",
    "data_1 = data_1.merge(user_unique,on=\"user_id\",how=\"left\")\n",
    "\n",
    "data_1 = data_1.rename(columns={\"product_id_y\":\"user_unique_review\",\n",
    "                                \"product_id_x\":\"product_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User review Density\n",
    "data_1[\"user_review_density\"] = data_1[\"num_review_user\"]/data_1[\"user_unique_review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restaurant review density\n",
    "data_1[\"prod_review_density\"] = data_1[\"num_review_prod\"]/data_1[\"prod_unique_review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max time range between user's reviews\n",
    "user_max_df = pd.DataFrame(data_1.groupby(\"user_id\")[\"date\"].max()).reset_index()\n",
    "user_min_df = pd.DataFrame(data_1.groupby(\"user_id\")[\"date\"].min()).reset_index()\n",
    "\n",
    "user_diff_df = user_max_df.merge(user_min_df, on=\"user_id\", how='inner')\n",
    "\n",
    "user_diff_df[\"diff\"] = (user_diff_df[\"date_x\"] - user_diff_df[\"date_y\"])\n",
    "user_diff_df[\"diff\"] = user_diff_df[\"diff\"].apply(lambda x: x.days)\n",
    "\n",
    "data_1 = data_1.merge(user_diff_df,on=\"user_id\",how=\"left\")\n",
    "\n",
    "data_1.drop([\"date_x\",\"date_y\"],axis=1,inplace=True)\n",
    "data_1 = data_1.rename(columns={\"diff\":\"user_time_max\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max time range between restaurant's reviews\n",
    "prod_max_df = pd.DataFrame(data_1.groupby(\"product_id\")[\"date\"].max()).reset_index()\n",
    "prod_min_df = pd.DataFrame(data_1.groupby(\"product_id\")[\"date\"].min()).reset_index()\n",
    "\n",
    "prod_diff_df = prod_max_df.merge(prod_min_df, on=\"product_id\", how='inner')\n",
    "\n",
    "prod_diff_df[\"diff\"] = (prod_diff_df[\"date_x\"] - prod_diff_df[\"date_y\"])\n",
    "prod_diff_df[\"diff\"] = prod_diff_df[\"diff\"].apply(lambda x: x.days)\n",
    "\n",
    "data_1 = data_1.merge(prod_diff_df,on=\"product_id\",how=\"left\")\n",
    "\n",
    "data_1.drop([\"date_x\",\"date_y\"],axis=1,inplace=True)\n",
    "data_1 = data_1.rename(columns={\"diff\":\"prod_time_max\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_pickle(\"yelp_text_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation with Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.drop([\"tokens\",\"review\",\"review1\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_sentiment(sentence):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "    score = nltk_sentiment.polarity_scores(sentence)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sentiment =  data_1['review_corrected']\n",
    "\n",
    "nltk_results = [nltk_sentiment(row) for row in review_sentiment]\n",
    "results_df = pd.DataFrame(nltk_results)\n",
    "nltk_df = review_sentiment.to_frame().join(results_df)\n",
    "\n",
    "data_1 = data.merge(nltk_df, left_index=True, right_index=True, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.drop(\"review_corrected_y\", axis=1, inplace=True)\n",
    "data_1.rename(columns={\"review_corrected_x\":\"review_corrected\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consistency between the text review sentiment & the rating\n",
    "data_1[\"consistency\"] = data_1[\"rating\"]\\\n",
    "                       /(5*(data_1[\"compound\"]-data_1[\"compound\"].min())/(data_1[\"compound\"].max()-data_1[\"compound\"].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes a long time conduct sentiment analysis for a dataset this large, therefore it is saved as a pickle...\n",
    "data_1.to_pickle(\"yelp_sentimented.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[data_1.compound>0.9][[\"review_corrected\",\"compound\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[data_1.compound<-0.9][[\"review_corrected\",\"compound\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tagging:\n",
    "\n",
    "- Fake reviewers might be using more adjective or adverb etc to put more emphasis on the comment, therefore we utilize this information from the review text. Let's examine with this perspective to add more dimension to our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "data_1[\"tag_tuples\"] = data_1[\"cleaned_tokens\"].apply(lambda x: nltk.pos_tag(x))\n",
    "data_1[\"pos_tags\"] = data_1[\"tag_tuples\"].apply(lambda x: [element[1] for element in x])\n",
    "\n",
    "data_1[\"pos_tag_dict\"] = data_1[\"pos_tags\"].apply(lambda x: nltk.FreqDist(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cols(df, liste):\n",
    "    for element in liste:\n",
    "        df[element] = df[\"pos_tag_dict\"].apply(lambda x: x[element])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = ['$',\"CC\",\"CD\",\"DT\",\"EX\",\"FW\",\"IN\",\"JJ\",\"JJR\",\"JJS\",\"LS\",\"MD\",\"NN\",\"NNP\",\"NNPS\",\"NNS\",\"PDT\",\"POS\",\n",
    "            \"PRP\",\"PRP$\",\"RB\",\"RBR\",\"RBS\",\"RP\",\"SYM\",\"TO\",\"UH\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\",\"WDT\",\"WP\",\"WP$\",\"WRB\"]\n",
    "\n",
    "data_1 = create_cols(data_1, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"all_types_sum\"] = data_1[['$', 'CC',\n",
    "       'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN',\n",
    "       'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS',\n",
    "       'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT',\n",
    "       'WP', 'WP$', 'WRB',]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"noun_perc\"] = data_1[['NN','NNP', 'NNPS', 'NNS']].sum(axis=1) / data_1[\"all_types_sum\"]\n",
    "\n",
    "data_1[\"adjective_perc\"] = data_1[['JJ', 'JJR', 'JJS']].sum(axis=1)/ data_1[\"all_types_sum\"]\n",
    "\n",
    "data_1[\"adverb_perc\"] = data_1[[ 'RB', 'RBR', 'RBS','RP']].sum(axis=1)/ data_1[\"all_types_sum\"]\n",
    "\n",
    "data_1[\"verb_perc\"] = data_1[['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']].sum(axis=1)/ data_1[\"all_types_sum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.drop([\"tag_tuples\",\"pos_tags\",\"pos_tag_dict\"], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_pickle(\"yelp_tagged.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud Analysis:\n",
    "\n",
    "- We are already considering and deriving effects from the text area, but by simply looking at a word cloud may generate additional ideas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRUE Review Word Cloud\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords.words('english'), \n",
    "                min_font_size = 10).generate(\" \".join(data_1[data_1['label'] == 0].review_corrected)) \n",
    "  \n",
    "# plot the word cloud for true review data                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FAKE Review Word Cloud\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords.words('english'), \n",
    "                min_font_size = 10).generate(\" \".join(data_1[data_1['label'] == 1].review_corrected)) \n",
    "  \n",
    "# plot the word cloud for fake review data                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Examination:\n",
    "\n",
    "- Let's see the effects we are looking for with all these recently created variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = data_1[data_1.label==1].reset_index(drop=True)\n",
    "true_data = data_1[data_1.label==0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_color = ()\n",
    "fake_color = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(data, feature, ax):\n",
    "    grouped_data = data.groupby(\"label\")[feature].mean()\n",
    "    bar = group_data.plot(kind = \"barh\", ax = ax)\n",
    "    \n",
    "    ax.set_facecolor('white')\n",
    "    ax.set_xlabel(feature, rotation = 0)\n",
    "    ax.set_ylabel(\"Target\", rotation = 90)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot(data, feature, ax, bins, color):\n",
    "    data[feature].hist(ax=ax, bins = bins)\n",
    "    \n",
    "    ax.set_facecolor('white')\n",
    "    ax.set_xlabel(feature, rotation = 0)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_look = ['len_review','neg', 'neu', 'pos', 'compound',\n",
    "                    'consistency', 'avg_rating_prod', 'avg_rating_user',\n",
    "                    'num_review_prod','num_review_user', 'user_time_max',\n",
    "                    'prod_time_max','user_unique_review', 'prod_unique_review',\n",
    "                    'user_review_density', 'prod_review_density',\"adjective_perc\",\n",
    "                    \"adverb_perc\",\"noun_perc\",\"verb_perc\"]\n",
    "\n",
    "n_of_rows = len(features_to_look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_of_rows, 3)\n",
    "bins = 10\n",
    "\n",
    "for i, feature in enumerate(features_to_look):\n",
    "    \n",
    "    bar_plot(data_1, feature, axes[i,0])\n",
    "    hist_plot(true_data, feature, axes[i,1], bins, color)\n",
    "    hist_plot(fake_data, feature, axes[i,2], bins, color)\n",
    "    \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Phase:\n",
    "\n",
    "- Let's start with conventional models, and following these, we will also try models based on text areas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = data_1[['label', 'len_review','neg', 'neu', 'pos', 'compound',\n",
    "       'consistency', 'avg_rating_prod', 'avg_rating_user', 'num_review_prod',\n",
    "       'num_review_user', '$', 'CC',\n",
    "       'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN',\n",
    "       'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS',\n",
    "       'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT',\n",
    "       'WP', 'WP$', 'WRB', 'user_time_max', 'prod_time_max',\n",
    "       'user_unique_review', 'prod_unique_review', 'user_review_density',\n",
    "       'prod_review_density',\"adjective_perc\",\"adverb_perc\",\"noun_perc\",\"verb_perc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"neg\"] = (final_data[\"neg\"]-final_data[\"neg\"].min())/(final_data[\"neg\"].max()-final_data[\"neg\"].min())\n",
    "final_data[\"neu\"] = (final_data[\"neg\"]-final_data[\"neg\"].min())/(final_data[\"neg\"].max()-final_data[\"neg\"].min())\n",
    "final_data[\"pos\"] = (final_data[\"neg\"]-final_data[\"neg\"].min())/(final_data[\"neg\"].max()-final_data[\"neg\"].min())\n",
    "final_data[\"compound\"] = (final_data[\"neg\"]-final_data[\"neg\"].min())/(final_data[\"neg\"].max()-final_data[\"neg\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.dropna(inplace=True)\n",
    "final_data.to_pickle(\"yelp_final_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of target variable:\n",
    "\n",
    "- In line with this, our study might require up/down sampling techniques..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how label is distributed:\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.countplot(final_data[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train - Test Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = final_data[\"label\"]\n",
    "x = final_data.drop(\"label\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upsampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "X = pd.concat([train_x, train_y], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "true = X[X.label==0]\n",
    "fake = X[X.label==1]\n",
    "\n",
    "# upsample minority\n",
    "fake_upsampled = resample(fake,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(true)/2), # match number in majority class\n",
    "                          random_state=123) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([true, fake_upsampled])\n",
    "\n",
    "train_yu = upsampled.label\n",
    "train_xu = upsampled.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Downsampling 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "X = pd.concat([train_x, train_y], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "true = X[X.label==0]\n",
    "fake = X[X.label==1]\n",
    "\n",
    "# upsample minority\n",
    "true_downsampled = resample(true,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(fake), # match number in majority class\n",
    "                          random_state=123) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "downsampled = pd.concat([fake, true_downsampled])\n",
    "\n",
    "train_yd = downsampled.label\n",
    "train_xd = downsampled.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Downsampling 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "X = pd.concat([train_x, train_y], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "true = X[X.label==0]\n",
    "fake = X[X.label==1]\n",
    "\n",
    "# upsample minority\n",
    "true_downsampled = resample(true,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(fake)*1.5), # match number in majority class\n",
    "                          random_state=123) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "downsampled = pd.concat([fake, true_downsampled])\n",
    "\n",
    "train_yd2 = downsampled.label\n",
    "train_xd2 = downsampled.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "def consufion_plot(y_test,prediction):\n",
    "\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(y_test, prediction),\n",
    "                                    show_absolute=True,\n",
    "                                    show_normed=True,\n",
    "                                    colorbar=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree - original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(train_x,train_y)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred1 = clf.predict(test_x)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(test_x)[:,1]\n",
    "\n",
    "consufion_plot(test_y, y_pred1)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred1))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred1))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred1))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred1))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree - downsampled 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(train_xd,train_yd)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred1 = clf.predict(test_x)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(test_x)[:,1]\n",
    "\n",
    "consufion_plot(test_y, y_pred1)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred1))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred1))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred1))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred1))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree - downsampled 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(train_xd2,train_yd2)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred1 = clf.predict(test_x)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(test_x)[:,1]\n",
    "\n",
    "consufion_plot(test_y, y_pred1)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred1))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred1))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred1))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred1))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree - upsampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(train_xu,train_yu)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred1 = clf.predict(test_x)\n",
    "\n",
    "y_pred_prob = clf.predict_proba(test_x)[:,1]\n",
    "\n",
    "consufion_plot(test_y, y_pred1)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred1))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred1))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred1))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred1))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc = rfc.fit(train_x,train_y)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred3 = rfc.predict(test_x)\n",
    "y_pred_proba = rfc.predict_proba(test_x)\n",
    "\n",
    "consufion_plot(test_y, y_pred3)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred3))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred3))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred3))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred3))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - downsampled 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc = rfc.fit(train_xd,train_yd)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred3 = rfc.predict(test_x)\n",
    "y_pred_proba = rfc.predict_proba(test_x)\n",
    "\n",
    "consufion_plot(test_y, y_pred3)\n",
    "print(classification_report(test_y, y_pred3))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred3))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred3))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred3))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred3))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - downsampled 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc = rfc.fit(train_xd2,train_yd2)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred3 = rfc.predict(test_x)\n",
    "y_pred_proba = rfc.predict_proba(test_x)\n",
    "\n",
    "consufion_plot(test_y, y_pred3)\n",
    "print(classification_report(test_y, y_pred3))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred3))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred3))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred3))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred3))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - upsampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc = rfc.fit(train_xu,train_yu)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred3 = rfc.predict(test_x)\n",
    "y_pred_proba = rfc.predict_proba(test_x)\n",
    "\n",
    "consufion_plot(test_y, y_pred3)\n",
    "print(classification_report(test_y, y_pred3))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred3))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred3))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred3))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred3))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes - original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(train_x, train_y)\n",
    "predicted= clf.predict(test_x)\n",
    "consufion_plot(test_y, predicted)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(test_y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes - downsampled 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(train_xd, train_yd)\n",
    "predicted= clf.predict(test_x)\n",
    "consufion_plot(test_y, predicted)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(test_y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes - downsampled 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(train_xd2, train_yd2)\n",
    "predicted= clf.predict(test_x)\n",
    "consufion_plot(test_y, predicted)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(test_y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes - upsampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(train_xu, train_yu)\n",
    "predicted= clf.predict(test_x)\n",
    "consufion_plot(test_y, predicted)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(test_y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost - original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb = xgb.fit(train_x,train_y)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred3 = xgb.predict(test_x)\n",
    "y_pred_proba = xgb.predict_proba(test_x)\n",
    "\n",
    "consufion_plot(test_y, y_pred3)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred3))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred3))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred3))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred3))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost - downsampled 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb = xgb.fit(train_xd,train_yd)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred3 = xgb.predict(test_x)\n",
    "y_pred_proba = xgb.predict_proba(test_x)\n",
    "\n",
    "consufion_plot(test_y, y_pred3)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred3))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred3))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred3))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred3))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost - downsampled 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb = xgb.fit(train_xd2,train_yd2)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred3 = xgb.predict(test_x)\n",
    "y_pred_proba = xgb.predict_proba(test_x)\n",
    "\n",
    "consufion_plot(test_y, y_pred3)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred3))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred3))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred3))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred3))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost - upsampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb = xgb.fit(train_xu,train_yu)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred3 = xgb.predict(test_x)\n",
    "y_pred_proba = xgb.predict_proba(test_x)\n",
    "\n",
    "consufion_plot(test_y, y_pred3)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, y_pred3))\n",
    "print (\"AUC Score:\", roc_auc_score(test_y, y_pred3))\n",
    "print (\"AUC Score prob:\", roc_auc_score(test_y, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(test_y, y_pred3))\n",
    "print (\"Recall:\", recall_score(test_y, y_pred3))\n",
    "print (\"F1 Score:\", f1_score(test_y, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL Trials with Text Area:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True, stop_words='english', ngram_range = (2,2), tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(data_1['review_corrected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, data_1['label'], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer()\n",
    "text_tf = tf.fit_transform(data_1['review_corrected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_tf, data_1['label'], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "\n",
    "consufion_plot(y_test, predicted)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF combined with our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(ngram_range=(3,3),max_features= 15000)\n",
    "text_tf = tf.fit_transform(data1['review_corrected'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tf, data1['label'], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = sp.sparse.hstack((X_train, train_x.values),format='csr')\n",
    "final_test = sp.sparse.hstack((X_test, test_x.values),format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(final_train, y_train)\n",
    "predicted= clf.predict(final_test)\n",
    "consufion_plot(y_test, predicted)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
